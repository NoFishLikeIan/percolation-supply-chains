\documentclass[../../main.tex]{subfiles}
\begin{document}

\section{Notation}

\begin{enumerate}
    \item For some integer $m$, let $[m] \coloneqq \{0, 1, \ldots, m\}$.
    \item For two numbers $x, s$ let the falling factorial of $x$ to $s$ be \begin{equation}
        (x)_s \coloneqq \begin{cases} x(x - 1)(x - 2)\ldots(x - (s - 1)) & \text{if } s \in \mathbb{N}_0 \\
       \Gamma(1 + x) \Big/ \Gamma(1 + x - s)& \text{if } s \in \Re^{+}
        \end{cases}
    \end{equation} and the rising factorial of $x$ to $s$ be \begin{equation}
        x^{(s)} \coloneqq \begin{cases} x(x + 1)(x + 2)\ldots(x + (s - 1)) & \text{if } s \in \mathbb{N}_0 \\
            \Gamma(1 + x + s) \Big/ \Gamma(1 + x)& \text{if } s \in \Re^{+}
             \end{cases}.
    \end{equation} This notation is a bit contentious since the Pochhammer symbol $(x)_s$ has been used to denote rising factorials, falling factorials, and binomial coefficients. The notation I chose is used in the Combinatorics and Graph Theory literature (\cite{harris_combinatorics_2008}).
\end{enumerate}

\section{Notes on the beta-binomial distribution}

\begin{enumerate}
    \item Usually, the beta-binomial is parametrized with two parameters $\alpha$ and $\beta$. Here I choose $f$ and $\rho$ instead, which provide a better interpretation for the problem at hand. The two parametrisations can be linked by taking \begin{equation}
        \rho = \frac{1}{\alpha + \beta + 1} \text{ and } \mu = \frac{\beta}{\alpha + \beta}.
    \end{equation}

    Other identities I will use extensively are

    \begin{equation}
        \alpha + \beta = \frac{1-\rho}{\rho} \text{ and } \beta = \mu \ \frac{1-\rho}{\rho}
    \end{equation}

    \item The beta-binomial can be seen as a compounded random variable, generated by taking a binomial distribution $F \sim \Bin(m, p)$ with probability of success $p \sim \Beta(\alpha, \beta)$.
    \item The first two moments of the beta-binomial distribution can be written as \begin{equation}
        \E[F] = m (1 - \mu) \text{ and } \V[F] = m \mu (1 - \mu) (1 + (m - 1) \rho).
    \end{equation} If $\rho = 0$, the beta-binomial degenerates into a binomial distribution, hence $\rho$ can be interpreted as an ``overdispersion'' vis-Ã -vis the binomial distribution.
    \item The probability mass function of the beta-binomial distribution is \begin{equation} \label{eq:pmf_betabinomial}
        f_F(k) = \binom{m}{k} \frac{B(k + \alpha, m - k + \beta)}{B(\alpha, \beta)}
    \end{equation} where $B$ is the beta function. 
    \item The moment generating function of the beta-binomial is \begin{equation} \label{eq:mgf_betabinomial}
        M_F(t) = \E\left[ e^{tF} \right] = \prescript{}{2}{F}_1(-m, \alpha, \alpha + \beta; 1 - e^{t}) = \sum^m_{n = 0} (-1)^n \binom{m}{n} \frac{B(\alpha + n, \beta)}{B(\alpha, \beta)} (1 - e^t)
    \end{equation}
\end{enumerate}


\section{Omitted Proofs}

\subsection{The Normalised Beta-Binomial Converges To A Beta Distribution}

\begin{lemma} \label{lemma:RisBeta}
    Take a r.v. $F_m \sim \Beta\Bin(m, \alpha, 
    \beta)$ and let the normalised beta-binomial be $R_m \coloneqq F_m / m$. Then

    \begin{equation}
        R_m \xrightarrow{d} R \sim \Beta(\alpha, \beta) \text{ as } m\rightarrow \infty. 
    \end{equation}
\end{lemma}

\begin{proof}
    The idea of the proof is to show that the moment generating function of $R_m$ converges pointwise to that of a beta distribution. This implies that the sequence $R_m$ converges in distribution to a beta, since the latter is determined by its moments (Theorem 30.2 in \cite{billingsley_probability_1995}). The moment generating function of $R_m$ can be written in terms of that of $F_m$  as 

    \begin{equation} \label{eq:mgf_rm}
        \begin{split}
            M_{R_m}(t) &= \E\left[e^{t R_m}\right] \\ 
            &= \E\left[e^{(t / m) F}\right] = M_{F_m}(t / m) \\
            &= \prescript{}{2}{F}_1(-m, \alpha, \alpha + \beta; 1 - e^{t / m}). 
        \end{split}
    \end{equation}

    We seek to prove that this converges pointwise to 

    \begin{equation}
        M_R(t) = \prescript{}{1}{F}_1(\alpha, \alpha + \beta, t).
    \end{equation}

    Consider the Taylor series of 

    \begin{equation}
        1 - e^{t / m} = -\frac{t}{m} - \sum^{\infty}_{k = 2} \frac{t^k}{m^k \ k!}.
    \end{equation}

    This and the fact that $\prescript{}{2}{F}_1$ is continuous in $\abs{t} < m$, allows us to write

    \begin{equation}
        \begin{split}
            \lim_{m \rightarrow \infty} M_{R_m}(t) &= \lim_{m \rightarrow \infty} \prescript{}{2}{F}_1(-m, \alpha, \alpha + \beta; 1 - e^{t / m})\\
            &= \lim_{m \rightarrow \infty} \prescript{}{2}{F}_1(-m, \alpha, \alpha + \beta; - t/ m) \\
            &= \lim_{m \rightarrow \infty}\sum^{\infty}_{n=0} \frac{\alpha^{(n)}}{(\alpha + \beta)^{(n)}} t^n \  \frac{(-m)^{(n)}}{(-m)^n}.
        \end{split}
    \end{equation}

    
    Consider the last term in the summand 

    \begin{equation}
        \begin{split}
            \frac{(-m)^{(n)}}{(-m)^n} &= \frac{(-m)(-m + 1)(-m + 2)\ldots(-m + n - 1)}{(-1)^n m^n} \\
            &= \frac{(-1)^n m^n + o((-m)^{m - 1})}{(-1)^n m^n} \xrightarrow{m\rightarrow \infty} 1.
        \end{split}
    \end{equation}

    Hence 

    \begin{equation}
        \lim_{m \rightarrow \infty} M_{R_m}(t) = \lim_{m \rightarrow \infty}\sum^{\infty}_{n=0} \frac{\alpha^{(n)}}{(\alpha + \beta)^{(n)}} t^n \  \frac{(-m)^{(n)}}{(-m)^n} = \sum^{\infty}_{n=0} \frac{\alpha^{(n)}}{(\alpha + \beta)^{(n)}} t^n = M_R(t)
    \end{equation}

    such that $R_m \xrightarrow{d} R$.
\end{proof}

\subsection{Power Of A Beta Distribution Follows A Beta Distribution}

\begin{lemma}
    Let $X \sim \Beta(\alpha, \beta)$. Then \begin{equation}
        X^s \sim \Beta(\alpha'_s, \beta'_s).
    \end{equation}
\end{lemma}

\begin{proof}
    The $k$-th moment of $X^s$ can be written as

    \begin{equation}
        \E\left[\left(X^s\right)^k\right] = \E\Big[X^{(sk)}\Big] = \frac{\alpha \ (\alpha + 1) \ldots (\alpha + s - 1) \ldots (\alpha + s k - 1) }{(\alpha + \beta) \ (\alpha + \beta + 1) \ldots (\alpha + \beta + s - 1) \ldots (\alpha + \beta + s k - 1) }
    \end{equation}

    Hence

    \begin{equation}
        \E\left[X^{(sk)}\right]
    \end{equation}

\end{proof}



\subsection[From distribution to probability]{Proof of Lemma (\ref{lemma:Ftop})}


\begin{proof}
    The Lemma stems from two assumptions. First
    \begin{equation}
        F_m \sim \Beta\Bin(m, \alpha, \beta),
    \end{equation}

    and second $s \in (0, \infty)$. Absuing notation, we can redefine $P_m$ to be a function of $R_m \coloneqq F_m / m$, particularly,
    
    \begin{equation}
        1 - P_m(s, R_m) = \frac{\big(m \ (1 - R_m)\big)_{(s)}}{\big( m \big)_{(s)}}.
    \end{equation}

    By considering only the leading terms in the falling factorial we have

    \begin{equation}
        \lim_{m \rightarrow \infty} 1 - P_m(s, R_m) = (1 - R)^s.
    \end{equation}

    By Lemma (\ref{lemma:RisBeta}), we know that,

    \begin{equation}
        (1 - R) \sim \Beta(\beta, \alpha).
    \end{equation}

\end{proof}

\newpage
\section{Appendix}

\subsection{Derivations}\label{appendix:derivations}

\notes{put stuff from inner-problem.tex here}

\subsection[Limits of Shift]{Limits of $G$}

Other limits of $G$ are

\begin{equation}
    \begin{split}
        \lim_{\rho \rightarrow 1} G(\mu, \rho, s) = (\mu, 0) \\
        \lim_{\mu \rightarrow 0} G(\mu, \rho, s) = (0, 0) \\
        \lim_{\mu \rightarrow 1} G(\mu, \rho, s) = (1, 1)
    \end{split}
\end{equation}

\subsection{Derivation of one dimensional system}

The agents' first order condition implies that

\begin{equation}
    \mu^{\tilde{s}} \log(\mu) = \kappa / \pi.
\end{equation}

This immediately allows to see that $\tilde{g}(\mu) = -\frac{\kappa / \pi}{\log(\mu)}$. The stability of a steady state $\bar{\mu}$ can be derived by

\begin{equation}
    \begin{split}
        1 &> \left.\frac{\partial \tilde{g}}{\partial \mu}\right\rvert_{\bar{\mu}} \\
        &> \frac{\kappa / \pi}{\log(\bar{\mu})^2 \bar{\mu}} \\
        &> \frac{\kappa / \pi}{\bar{\mu}}, \text{ by using } \bar{\mu} \log(\bar{\mu}) = -\frac{\kappa}{\pi}, \\
        \bar{\mu} \pi &> \kappa
    \end{split}
\end{equation}

\begin{figure}
    \inputTikZ{1}{../diagrams/probability-propagation.tikz}
\end{figure}

\end{document}